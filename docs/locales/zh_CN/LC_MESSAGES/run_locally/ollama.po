# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-18 21:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/run_locally/ollama.md:1 517dd77619b8438b93e089aaf72c0ad2
msgid "Ollama"
msgstr "Ollama"

#: ../../source/run_locally/ollama.md:3 13089a9172944dad96ea28917a956455
msgid "[Ollama](https://ollama.com/) helps you run LLMs locally with only a few commands. It is available at MacOS, Linux, and Windows. Now, Qwen2.5 is officially on Ollama, and you can run it with one command:"
msgstr "[Ollama](https://ollama.com/)帮助您通过少量命令即可在本地运行LLM。它适用于MacOS、Linux和Windows操作系统。现在，Qwen2.5正式上线Ollama，您只需一条命令即可运行它："

#: ../../source/run_locally/ollama.md:11 180dd973001948e6b7079372d1a85662
msgid "Next, we introduce more detailed usages of Ollama for running Qwen2.5 models."
msgstr "接着，我们介绍在Ollama使用Qwen2.5模型的更多用法"

#: ../../source/run_locally/ollama.md:13 f1a63d6d649848bab3665c95ba992e9a
msgid "Quickstart"
msgstr "快速开始"

#: ../../source/run_locally/ollama.md:15 4513e525a1594008b819b5b7f0dfe2bf
msgid "Visit the official website [Ollama](https://ollama.com/) and click download to install Ollama on your device. You can also search models on the website, where you can find the Qwen2.5 models. Except for the default one, you can choose to run Qwen2.5-Instruct models of different sizes by:"
msgstr "访问官方网站[Ollama](https://ollama.com/)，点击`Download`以在您的设备上安装Ollama。您还可以在网站上搜索模型，在这里您可以找到Qwen2.5系列模型。除了默认模型之外，您可以通过以下方式选择运行不同大小的Qwen2.5-Instruct模型："

#: ../../source/run_locally/ollama.md:19 680843291b7c4fe195373f7f85467862
msgid "`ollama run qwen2.5:0.5b`"
msgstr ""

#: ../../source/run_locally/ollama.md:20 99350ad755a24f91a34a327bcc7e48c5
msgid "`ollama run qwen2.5:1.5b`"
msgstr ""

#: ../../source/run_locally/ollama.md:21 8bd6a62153394e9a83dd12c29bec891a
msgid "`ollama run qwen2.5:3b`"
msgstr ""

#: ../../source/run_locally/ollama.md:22 8bd6a62153394e9a83dd12c29bec891a
msgid "`ollama run qwen2.5:7b`"
msgstr ""

#: ../../source/run_locally/ollama.md:23 99350ad755a24f91a34a327bcc7e48c5
msgid "`ollama run qwen2.5:14b`"
msgstr ""

#: ../../source/run_locally/ollama.md:24 7e11fc937a6c43d7940065048e5fa9b8
msgid "`ollama run qwen2.5:32b`"
msgstr ""

#: ../../source/run_locally/ollama.md:25 7e11fc937a6c43d7940065048e5fa9b8
msgid "`ollama run qwen2.5:72b`"
msgstr ""

#: ../../source/run_locally/ollama.md:28 f70517b9179b4569a3f0320c3669c5ac
msgid "`ollama` does not host base models. Even though the tag may not have the instruct suffix, they are all instruct models."
msgstr "`ollama`并不托管基模型。即便模型标签不带instruct后缀，实际也是instruct模型。"

#: ../../source/run_locally/ollama.md:32 da336b53d9d54738802906db7246cd2e
msgid "Run Ollama with Your GGUF Files"
msgstr "用Ollama运行你自己的GGUF文件"

#: ../../source/run_locally/ollama.md:34 a45b6bcaab944f00ae23384aaf4bebfe
msgid "Sometimes you don't want to pull models and you just want to use Ollama with your own GGUF files. Suppose you have a GGUF file of Qwen2.5, `qwen2.5-7b-instruct-q5_0.gguf`. For the first step, you need to create a file called `Modelfile`. The content of the file is shown below:"
msgstr "有时您可能不想拉取模型，而是希望直接使用自己的GGUF文件来配合Ollama。假设您有一个名为`qwen2.5-7b-instruct-q5_0.gguf`的Qwen2.5的GGUF文件。在第一步中，您需要创建一个名为`Modelfile``的文件。该文件的内容如下所示："

#: ../../source/run_locally/ollama.md:97 0300ccc8902641e689c5214717fb588d
msgid "Then create the ollama model by running:"
msgstr "然后通过运行下列命令来创建一个ollama模型"

#: ../../source/run_locally/ollama.md:103 5529e410e16f421eb291c1a048a6eb2a
msgid "Once it is finished, you can run your ollama model by:"
msgstr "完成后，你即可运行你的ollama模型："

#: ../../source/run_locally/ollama.md:109 56cc1331a12c4e46b25cb533f8511bf7
msgid "Tool Use"
msgstr "工具调用"

#: ../../source/run_locally/ollama.md:111 94381f70ed3b4b0d8b88ab7bf074f8fe
msgid "Tool use is now support Ollama and you should be able to run Qwen2.5 models with it. For more details, see our [function calling guide](../framework/function_call)."
msgstr "Ollama现已支持工具调用，Qwen2.5也已适配。更多详情，请参阅我们的[函数调用指南](../framework/function_call)"
