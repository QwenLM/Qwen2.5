# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-06 23:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/run_locally/ollama.rst:2 53de2a112ba54d39a197bdae1a19cd69
msgid "Ollama"
msgstr "Ollama"

#: ../../source/run_locally/ollama.rst:4 fde5c7a3f49a4d5eba64aaf756415d59
msgid ""
"`Ollama <https://ollama.com/>`__ helps you run LLMs locally with only a "
"few commands. It is available at MacOS, Linux, and Windows. Now, Qwen2 is"
" officially on Ollama, and you can run it with one command:"
msgstr ""
"`Ollama <https://ollama.com/>`__ "
"帮助您通过少量命令即可在本地运行LLM。它适用于MacOS、Linux和Windows操作系统。现在，Qwen2正式上线Ollama，您只需一条命令即可运行它："

#: ../../source/run_locally/ollama.rst:12 99348b96c61c43be933016df746b9f65
msgid ""
"Next, we introduce more detailed usages of Ollama for running Qwen2 "
"models."
msgstr "接着，我们介绍在Ollama使用Qwen2模型的更多用法"

#: ../../source/run_locally/ollama.rst:16 6f36840d2dbc4dc991cbee4e88b989d5
msgid "Quickstart"
msgstr "快速开始"

#: ../../source/run_locally/ollama.rst:18 8ee18e0a54624598a79ab5fef01f436d
msgid ""
"Visit the official website `Ollama <https://ollama.com/>`__ and click "
"download to install Ollama on your device. You can also search models in "
"the website, where you can find the Qwen2 models. Except for the default "
"one, you can choose to run Qwen2-Instruct models of different sizes by:"
msgstr ""
"访问官方网站 `Ollama <https://ollama.com/>`__ ”，点击 ``Download`` "
"以在您的设备上安装Ollama。您还可以在网站上搜索模型，在这里您可以找到Qwen2系列模型。除了默认模型之外，您可以通过以下方式选择运行不同大小的Qwen2-Instruct模型："

#: ../../source/run_locally/ollama.rst:24 56057e178ecc4fe298a8a90794f956ee
msgid "``ollama run qwen2:0.5b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:25 56057e178ecc4fe298a8a90794f956ee
msgid "``ollama run qwen2:1.5b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:26 f1a84ddfe3a14f59b62f4dfc8e4c262f
msgid "``ollama run qwen2:7b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:27 e8c2c80bf29f4c588abf4a3f06a49c96
msgid "``ollama run qwen2:72b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:30 e7c09ad8871e4701b05baca042ca279c
msgid "Run Ollama with Your GGUF Files"
msgstr "在Ollama运行你的GGUF文件"

#: ../../source/run_locally/ollama.rst:32 089328f73e7d4382a9d82cd10a3c093e
msgid ""
"Sometimes you don't want to pull models and you just want to use Ollama "
"with your own GGUF files. Suppose you have a GGUF file of Qwen2, ``qwen2"
"-7b-instruct-q5_0.gguf``. For the first step, you need to create a file "
"called ``Modelfile``. The content of the file is shown below:"
msgstr ""
"有时您可能不想拉取模型，而是希望直接使用自己的GGUF文件来配合Ollama。假设您有一个名为 ``qwen2-7b-"
"instruct-q5_0.gguf`` 的Qwen2的GGUF文件。在第一步中，您需要创建一个名为 ``Modelfile`` "
"的文件。该文件的内容如下所示："

#: ../../source/run_locally/ollama.rst:59 f27b0fb22282407e86562603b88821f2
msgid "Then create the ollama model by running:"
msgstr "然后通过运行下列命令来创建一个ollama模型"

#: ../../source/run_locally/ollama.rst:65 1015f2be7dca46489ef288f98f80527d
msgid "Once it is finished, you can run your ollama model by:"
msgstr "完成后，你即可运行你的ollama模型："
