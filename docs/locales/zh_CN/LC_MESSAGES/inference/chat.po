# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-06 19:37+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/inference/chat.rst:2 401ded17c1eb407e9a2c2df1c2b3260c
msgid "Using Transformers to Chat"
msgstr "使用Transformers实现Chat"

#: ../../source/inference/chat.rst:4 621b6a5183214027883054a19513ca7f
#, fuzzy
msgid ""
"The most significant but also the simplest usage of Qwen2 is to chat with"
" it using the ``transformers`` library. In this document, we show how to "
"chat with ``Qwen2-7B-Instruct``, in either streaming mode or not."
msgstr ""
"使用Qwen2最简单的方法就是利用``transformers``库与之对话。"
"在本文档中，我们将展示如何在流式模式或非流式模式下与Qwen2"
"-7B-Instruct进行对话。"

#: ../../source/inference/chat.rst:9 6f0045c90e4a4c17b46c918747ef8ffb
msgid "Basic Usage"
msgstr "基本用法"

#: ../../source/inference/chat.rst:11 5220514bc84f4495a5215a294ac93465
#, fuzzy
msgid ""
"You can just write several lines of code with ``transformers`` to chat "
"with Qwen2-Instruct. Essentially, we build the tokenizer and the model "
"with ``from_pretrained`` method, and we use ``generate`` method to "
"perform chatting with the help of chat template provided by the "
"tokenizer. Below is an example of how to chat with Qwen2-7B-Instruct:"
msgstr ""
"你只需借助 ``transformers`` "
"库编写几行代码，就能与Qwen2-Instruct进行对话。实质上，我们通过``from_pretrained``方法构建tokenizer和模型，然后利用 "
"``generate`` 方法，在tokenizer提供的chat template的辅助下进行对话。以下是一个如何与Qwen2-7B-"
"Instruct进行对话的示例："

#: ../../source/inference/chat.rst:56 31d4b15b63894e4c8897e4658d6510f0
msgid ""
"If you would like to apply Flash Attention 2, you can load the model as "
"shown below:"
msgstr "如果你想使用Flash Attention 2，你可以用下面这种方式读取模型："

#: ../../source/inference/chat.rst:67 e7c6476ff9ce4f08a6b9cc4cbe311df8
msgid ""
"Note that the previous method in the original Qwen repo ``chat()`` is now"
" replaced by ``generate()``. The ``apply_chat_template()`` function is "
"used to convert the messages into a format that the model can understand."
" The ``add_generation_prompt`` argument is used to add a generation "
"prompt, which refers to ``<|im_start|>assistant\\n`` to the input. "
"Notably, we apply ChatML template for chat models following our previous "
"practice. The ``max_new_tokens`` argument is used to set the maximum "
"length of the response. The ``tokenizer.batch_decode()`` function is used"
" to decode the response. In terms of the input, the above ``messages`` is"
" an example to show how to format your dialog history and system prompt. "
"By default, if you do not specify system prompt, we directly use ``You "
"are a helpful assistant.``."
msgstr ""
"请注意，原Qwen仓库中的旧方法 ``chat()`` 现在已被 ``generate()`` 方法替代。这里使用了 "
"``apply_chat_template()`` 函数将消息转换为模型能够理解的格式。其中的 ``add_generation_prompt``"
" 参数用于在输入中添加生成提示，该提示指向 ``<|im_start|>assistant\\n`` "
"。尤其需要注意的是，我们遵循先前实践，对chat模型应用ChatML模板。而 ``max_new_tokens`` "
"参数则用于设置响应的最大长度。此外，通过 ``tokenizer.batch_decode()`` 函数对响应进行解码。关于输入部分，上述的 "
"``messages`` 是一个示例，展示了如何格式化对话历史记录和系统提示。默认情况下，如果您没有指定系统提示，我们将直接使用 ``You "
"are a helpful assistant.`` 作为系统提示。"

#: ../../source/inference/chat.rst:81 173e7c1b64a64aa5b257d21f3276ac6d
msgid "Streaming Mode"
msgstr "流式输出"

#: ../../source/inference/chat.rst:83 795763a62cb7457689725504e691e049
msgid ""
"With the help of ``TextStreamer``, you can modify your chatting with Qwen"
" to streaming mode. Below we show you an example of how to use it:"
msgstr "借助 ``TextStreamer`` ，您可以将与Qwen的对话切换到流式传输模式。下面是一个关于如何使用它的示例："

#: ../../source/inference/chat.rst:100 87b1b72ef466431ba3f7453c9f386f69
msgid ""
"Besides using ``TextStreamer``, we can also use ``TextIteratorStreamer`` "
"which stores print-ready text in a queue, to be used by a downstream "
"application as an iterator:"
msgstr ""
"除了使用 ``TextStreamer`` 之外，我们还可以使用 ``TextIteratorStreamer`` "
"，它将可打印的文本存储在一个队列中，以便下游应用程序作为迭代器来使用："

#: ../../source/inference/chat.rst:122 2ac04859fe2f4084b4d0f423ba76478c
msgid "Next Step"
msgstr "下一步"

#: ../../source/inference/chat.rst:124 3cc062eb2dd04c04bc3425ca2362d57c
#, fuzzy
msgid ""
"Now you can chat with Qwen2 in either streaming mode or not. Continue to "
"read the documentation and try to figure out more advanced usages of "
"model inference!"
msgstr "现在，你可以选择流式模式或非流式模式与Qwen2进行对话。继续阅读文档，并尝试探索模型推理的更多高级用法！\""

