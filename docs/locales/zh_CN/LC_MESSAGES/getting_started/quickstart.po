# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-06-13 16:36+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/getting_started/quickstart.md:1
#: a99b6a1db1374218a20b06bcd0c57957
msgid "Quickstart"
msgstr "快速开始"

#: ../../source/getting_started/quickstart.md:3
#: 1da6c3f04eb24db8b697e094163096a1
msgid "This guide helps you quickly start using Qwen3.  We provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment."
msgstr "本指南帮助您快速上手 Qwen3 的使用，并提供了如下示例： [Hugging Face Transformers](https://github.com/huggingface/transformers) 以及 [ModelScope](https://github.com/modelscope/modelscope) 和 [vLLM](https://github.com/vllm-project/vllm) 在部署时的应用实例。"

#: ../../source/getting_started/quickstart.md:6
#: 11c38e7141f941efb448e7099935b8a9
msgid "You can find Qwen3 models in [the Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) at Hugging Face Hub and [the Qwen3 collection](https://www.modelscope.cn/collections/Qwen3-9743180bdc6b48) at ModelScope."
msgstr "你可以在 Hugging Face Hub 的 [Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) 或 ModelScope 的 [Qwen3 collection](https://www.modelscope.cn/collections/Qwen3-9743180bdc6b48) 中寻找 Qwen3 模型。"

#: ../../source/getting_started/quickstart.md:8
#: 842c24eb7d30496baf9025af21ca1ed0
msgid "Transformers"
msgstr "Transformers"

#: ../../source/getting_started/quickstart.md:10
#: d0c61f87b0b347ae91e115ba60ebd46e
msgid "To get a quick start with Qwen3, you can try the inference with `transformers` first. Make sure that you have installed `transformers>=4.51.0`. We advise you to use Python 3.10 or higher, and PyTorch 2.6 or higher."
msgstr "要快速上手 Qwen3 ，我们建议您首先尝试使用 `transformers` 进行推理。请确保已安装了 `transformers>=4.51.0` 版本。我们建议您使用 Python 3.10 或以上版本， PyTorch 2.6 或以上版本。"

#: ../../source/getting_started/quickstart.md:14
#: a7fbb3d015b4440ca14ba00821f84fb0
msgid "The following is a very simple code snippet showing how to run Qwen3-8B:"
msgstr "以下是一个非常简单的代码片段示例，展示如何运行 Qwen3 模型："

#: ../../source/getting_started/quickstart.md:63
#: b55178516d31433c9ead5287e9abd3b4
msgid "Qwen3 will think before respond, similar to QwQ models. This means the model will use its reasoning abilities to enhance the quality of generated responses. The model will first generate thinking content wrapped in a `<think>...</think>` block, followed by the final response."
msgstr "Qwen3 将在实际回复前思考，与 QwQ 模型类似。这意味着模型将运用其推理能力来提升生成回复的质量。模型会首先生成包含在 `<think>...</think>` 块中的思考内容，随后给出最终回复。"

#: ../../source/getting_started/quickstart.md:67
#: e31fa076c2264d9fa2ae5d8516b7f4a7
msgid "Hard Switch: To strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models, you can set `enable_thinking=False` when formatting the text."
msgstr "硬开关：为了严格禁用模型的思考行为，使其功能与之前的Qwen2.5-Instruct模型保持一致，您可以在格式化文本时设置`enable_thinking=False`。"

#: ../../source/getting_started/quickstart.md:77
#: a6c11c7147e54cf9a86d65b4f80840c3
msgid "It can be particularly useful in scenarios where disabling thinking is essential for enhancing efficiency."
msgstr "在某些需要通过禁用思考来提升效率的场景中，这一功能尤其有用。"

#: ../../source/getting_started/quickstart.md:79
#: 57bd7b66fe3b4dbfabe7439dc67b7d5f
msgid "Soft Switch: Qwen3 also understands the user's instruction on its thinking behavior, in particular, the soft switch `/think` and `/no_think`. You can add them to user prompts or system messages to switch the model's thinking mode from turn to turn.  The model will follow the most recent instruction in multi-turn conversations."
msgstr "软开关：Qwen3 还能够理解用户对其思考行为的指令，特别是软开关 `/think` 和 `/no_think`。您可以将这些指令添加到用户 (user) 或系统 (system) 消息中，以在对话轮次之间灵活切换模型的思考模式。在多轮对话中，模型将遵循最近的指令。"

#: ../../source/getting_started/quickstart.md:85
#: 73fa1ee92c7b4a71a2724aedb665dd1b
msgid "For thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in `generation_config.json`). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.  For more detailed guidance, please refer to the Best Practices section."
msgstr "对于思考模式，使用 Temperature=0.6，TopP=0.95，TopK=20，以及 MinP=0（`generation_config.json` 中的默认设置）。不要使用贪婪解码，因为它可能导致性能下降和无尽的重复。更多详细指导，请参阅最佳实践部分。"

#: ../../source/getting_started/quickstart.md:89
#: 4ce3b1cf5e1349628a66c101432fd748
msgid "For non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0."
msgstr "对于非思考模式，我们建议使用 Temperature=0.7，TopP=0.8，TopK=20，以及 MinP=0。"

#: ../../source/getting_started/quickstart.md:93
#: 34d138d1ca8c4ecab4002b354cfe64d2
msgid "ModelScope"
msgstr "魔搭 (ModelScope)"

#: ../../source/getting_started/quickstart.md:95
#: 0c25fecd5e42412a80214fbc35b08226
msgid "To tackle with downloading issues, we advise you to try [ModelScope](https://github.com/modelscope/modelscope). Before starting, you need to install `modelscope` with `pip`."
msgstr "为了解决下载问题，我们建议您尝试从 [ModelScope](https://github.com/modelscope/modelscope) 进行下载。开始之前，需要使用 `pip` 安装 `modelscope` 。"

#: ../../source/getting_started/quickstart.md:98
#: d8ccf4eafeb849ae8bd49d9fb2281c60
msgid "`modelscope` adopts a programmatic interface similar (but not identical) to `transformers`. For basic usage, you can simply change the first line of code above to the following:"
msgstr "`modelscope` 采用了与 `transformers` 类似（但不完全一致）的编程接口。对于基础使用，仅需将上面代码第一行做如下修改："

#: ../../source/getting_started/quickstart.md:105
#: f17c53400572487ca066135facd711bb
msgid "For more information, please refer to [the documentation of `modelscope`](https://www.modelscope.cn/docs)."
msgstr "欲获取更多信息，请参考 [`modelscope` 文档](https://www.modelscope.cn/docs)。"

#: ../../source/getting_started/quickstart.md:107
#: 6e3028c50b2146bd932d168662e57620
msgid "OpenAI API Compatibility"
msgstr ""

#: ../../source/getting_started/quickstart.md:109
#: 046b0833e5b744cda72d7a7ef5672cc2
msgid "You can serve Qwen3 via OpenAI-compatible APIs using frameworks such as vLLM, SGLang, and interact with the API using common HTTP clients or the OpenAI SDKs."
msgstr ""

#: ../../source/getting_started/quickstart.md:112
#: 1542f3cbe7ba4adab50dfc85da110c36
msgid "Here we take Qwen3-8B as an example to start the API:"
msgstr ""

#: ../../source/getting_started/quickstart.md:114
#: 4bfa52cf82914d73b0fcbbceafa4ff8a
msgid "SGLang (`sglang>=0.4.6.post1` is required):"
msgstr ""

#: ../../source/getting_started/quickstart.md:120
#: cc7a9ae4d4fe4b86be41b376d7334024
msgid "vLLM (`vllm>=0.8.5` is recommended):"
msgstr ""

#: ../../source/getting_started/quickstart.md:126
#: 7a690ab7e23f4505a355abd50e647101
msgid "Then, you can use the [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) to communicate with Qwen:"
msgstr "然后，可以使用 [\"create chat\" interface](https://platform.openai.com/docs/api-reference/chat/completions/create>) 来与 Qwen 进行交流："

#: ../../source/getting_started/quickstart.md 9cd82768a97142acac1c2c63a05e1ad3
msgid "curl"
msgstr ""

#: ../../source/getting_started/quickstart.md 7d220176fa6b4622ae74431873d49396
msgid "Python"
msgstr ""

#: ../../source/getting_started/quickstart.md:146
#: f55afdfc6fa54f468b1b554fab082263
msgid "You can use the API client with the `openai` Python SDK as shown below:"
msgstr "您可以按照下面所示的方式，使用 `openai` Python SDK中的客户端："

#: ../../source/getting_started/quickstart.md:175
#: d0370e64550a4c51b6146ad7dfb52f97
msgid "While the soft switch is always available, the hard switch is also available in the API through the following configuration to the API call. For more usage, please refer to our document on [SGLang](../deployment/sglang) and [vLLM](../deployment/vllm)."
msgstr "虽然软开关始终可用，但硬开关也可以通过以下 API 调用配置在 API 中使用。更多用法，请参阅我们关于 [SGLang](../deployment/sglang) 和 [vLLM](../deployment/vllm) 的文档。"

#: ../../source/getting_started/quickstart.md:178
#: e48c576eba5e4c5db9ef0a48882e18c2
msgid "Thinking Budget"
msgstr "思考预算"

#: ../../source/getting_started/quickstart.md:180
#: cc5be59588e34164b7f2e84a7d8b82c0
msgid "Qwen3 supports the configuration of thinking budget. It is achieved by ending the thinking process once the budget is reached and guiding the model to generate the \"summary\" with an early-stopping prompt."
msgstr "Qwen3 支持配置思考预算。其实现方式是，一旦达到预算，便结束思考过程，并通过提前停止提示引导模型生成“总结”。"

#: ../../source/getting_started/quickstart.md:183
#: 8a2760351bc04093adc5017b58ec4981
msgid "Since this feature involves customization specific to each model, it is currently not available in the open-source frameworks and only implemented by [the Alibaba Cloud Model Studio API](https://www.alibabacloud.com/help/en/model-studio/deep-thinking#6f0633b9cdts1)."
msgstr "由于此功能涉及针对模型的定制，目前在开源框架中不可用，仅由[阿里云百炼API](https://bailian.console.aliyun.com/?tab=doc#/doc/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2870973.html&renderType=iframe)实现。" 

#: ../../source/getting_started/quickstart.md:185
#: 01acc4a8caa443dc8da81862d2e7d6bb
msgid "However, with existing open-source frameworks, one can generate twice to implement this feature as follows:"
msgstr "然而，利用现有的开源框架，可以通过两次生成来实现此功能，具体如下："

#: ../../source/getting_started/quickstart.md:186
#: 677f5e6ad5b1416aa4bbadff3e6537a1
msgid "For the first time, generate tokens up to the thinking budget and check if the thinking process is finished. If the thinking process is not finished, append the early-stopping prompt."
msgstr "第一次生成时，生成的token数量达到思考预算，并检查思考过程是否完成。如果思考过程未完成，则追加提前停止提示。"

#: ../../source/getting_started/quickstart.md:187
#: 479c92c1bc9d4a74b18d5c175d1e6cda
msgid "For the second time, continue generation until the end of the content or the upper length limit is fulfilled."
msgstr "第二次生成时，继续生成直到内容结束或达到长度上限。"

#: ../../source/getting_started/quickstart.md:189
#: dd7834e30f9344ddad21792279ad4732
msgid "The following snippet shows the implementation with Hugging Face Transformers:"
msgstr "以下代码片段展示了使用Hugging Face Transformers的实现："

#: ../../source/getting_started/quickstart.md:262
#: 85f448fb112e4742bc5d4bd05979f30d
msgid "You should see the output in the console like the following"
msgstr "您应该会在控制台中看到类似以下的输出："

#: ../../source/getting_started/quickstart.md:274
#: 4fc4ae8cdbb54c8b97b2da717d61e42e
msgid "For purpose of demonstration only, `thinking_budget` is set to 16. However, `thinking_budget` should not be set to that low in practice. We recommend tuning `thinking_budget` based on the latency users can accept and setting it higher than 1024 for meaningful improvements across tasks."
msgstr "出于示例目的，`thinking_budget` 被设置为 16。然而，在实际应用中不应将其设置得如此低。我们建议根据用户可接受的延迟调整 `thinking_budget`，并将其设置为高于 1024，以在各项任务中获得有意义的改进。"

#: ../../source/getting_started/quickstart.md:278
#: f5b4018640c24a3cb6dd1e958f2860d9
msgid "If thinking is not desired at all, developers should make use of the hard switch instead."
msgstr "如果完全不需要思考，开发者应改用硬开关。"

#: ../../source/getting_started/quickstart.md:281
#: a6bbbe9da06c43c09cd3756e075e6103
msgid "Next Step"
msgstr "下一步"

#: ../../source/getting_started/quickstart.md:283
#: 40f5270a92bf490ab4fa0d6af2761044
msgid "Now, you can have fun with Qwen3 models.  Would love to know more about its usage?  Feel free to check other documents in this documentation."
msgstr "现在，您可以尽情探索 Qwen3 模型的各种用途。若想了解更多，请随时查阅本文档中的其他内容。"
