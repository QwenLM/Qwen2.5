# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-08-08 19:58+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/training/SFT/llama_factory.rst:2
#: 68febcbbc0494a58a6f7847621f43c8a
msgid "LLaMA-Factory"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:4
#: eb81bdea49a044768076a3b6d5b96c6b
msgid ""
"Here we provide a script for supervised finetuning Qwen2 with `LLaMA-"
"Factory <https://github.com/hiyouga/LLaMA-Factory>`__. This script for "
"supervised finetuning (SFT) has the following features:"
msgstr ""
"我们将介绍如何使用 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ "
"微调Qwen2模型。本脚本包含如下特点："

#: ../../source/training/SFT/llama_factory.rst:8
#: 35fa1b5c50b2481993528c89ba866607
msgid "Support single-GPU and multi-GPU training;"
msgstr "支持单卡和多卡分布式训练"

#: ../../source/training/SFT/llama_factory.rst:10
#: 3b87bf3c51b14c0a816de5b651073412
msgid "Support full-parameter tuning, LoRA, Q-LoRA, Dora."
msgstr "支持全参数微调、LoRA、Q-LoRA 和 DoRA 。"

#: ../../source/training/SFT/llama_factory.rst:12
#: d0f877c79ad940379e6fb76efd3252f5
msgid "In the following, we introduce more details about the usage of the script."
msgstr "下文将介绍更多关于脚本的用法。"

#: ../../source/training/SFT/llama_factory.rst:16
#: e7ccffc6e55441dc8f3167113a57b448
msgid "Installation"
msgstr "安装"

#: ../../source/training/SFT/llama_factory.rst:18
#: c830b89e259e49efa3278df0203b00ad
msgid "Before you start, make sure you have installed the following packages:"
msgstr "开始之前，确保你已经安装了以下代码库："

#: ../../source/training/SFT/llama_factory.rst:20
#: 180689ab6ba04fe094e9270f1c038b5a
msgid ""
"Follow the instructions of `LLaMA-Factory <https://github.com/hiyouga"
"/LLaMA-Factory>`__, and build the environment."
msgstr ""
"根据 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ "
"官方指引构建好你的环境"

#: ../../source/training/SFT/llama_factory.rst:23
#: 212a562d08aa438f975ab72d50b2e52b
msgid "Install these packages (Optional):"
msgstr "安装下列代码库（可选）："

#: ../../source/training/SFT/llama_factory.rst:30
#: 5f717bfa34774573a5ce49feedf3c6b6
msgid ""
"If you want to use `FlashAttention-2 <https://github.com/Dao-AILab/flash-"
"attention>`__, make sure your CUDA is 11.6 and above."
msgstr ""
"如你使用 `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`__"
"  ，请确保你的CUDA版本在11.6以上。"

#: ../../source/training/SFT/llama_factory.rst:35
#: 7f60c94192c04cb6a1d22be116518f08
msgid "Data Preparation"
msgstr "准备数据"

#: ../../source/training/SFT/llama_factory.rst:37
#: 21161fc7605847d4b94aa6bbf40f288e
msgid ""
"LLaMA-Factory provides several training datasets in ``data`` folder, you "
"can use it directly. If you are using a custom dataset, please prepare "
"your dataset as follows."
msgstr ""
"LLaMA-Factory 在 ``data`` "
"文件夹中提供了多个训练数据集，您可以直接使用它们。如果您打算使用自定义数据集，请按照以下方式准备您的数据集。"

#: ../../source/training/SFT/llama_factory.rst:41
#: 1146f8d06b8f447fb8b42bbaab3d01f1
msgid ""
"Organize your data in a **json** file and put your data in ``data`` "
"folder. LLaMA-Factory supports dataset in ``alpaca`` or ``sharegpt`` "
"format."
msgstr ""
"请将您的数据以 ``json`` 格式进行组织，并将数据放入 data 文件夹中。LLaMA-Factory 支持以 ``alpaca`` 或 "
"``sharegpt`` 格式的数据集。"

#: ../../source/training/SFT/llama_factory.rst:45
#: 3706d5d7db8540aa9ead87365ee8c483
msgid "The dataset in ``alpaca`` format should follow the below format:"
msgstr "``alpaca`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:62
#: 92f408c7fa894709ad1d5b9792320362
msgid "The dataset in ``sharegpt`` format should follow the below format:"
msgstr "``sharegpt`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:83
#: e1e27d0afbce48119c7a2153cbdaabf7
msgid ""
"Provide your dataset definition in ``data/dataset_info.json`` in the "
"following format ."
msgstr "在 ``data/dataset_info.json`` 文件中提供您的数据集定义，并采用以下格式："

#: ../../source/training/SFT/llama_factory.rst:86
#: 711bfaba832c48f28464d57a1f7d1169
msgid ""
"For ``alpaca`` format dataset, the columns in ``dataset_info.json`` "
"should be:"
msgstr "对于 ``alpaca`` 格式的数据集，其 ``dataset_info.json`` 文件中的列应为："

#: ../../source/training/SFT/llama_factory.rst:102
#: 544cc33ea8f84e45b887887466a6cd1b
msgid ""
"For ``sharegpt`` format dataset, the columns in ``dataset_info.json`` "
"should be:"
msgstr "对于 ``sharegpt`` 格式的数据集，``dataset_info.json`` 文件中的列应该包括："

#: ../../source/training/SFT/llama_factory.rst:124
#: 3d7da197c0394832a6278df32f39a1fb
msgid "Training"
msgstr "训练"

#: ../../source/training/SFT/llama_factory.rst:126
#: 8f894e07af7e4c60bff6b6011e0b03e0
msgid "Execute the following training command:"
msgstr "执行下列命令："

#: ../../source/training/SFT/llama_factory.rst:166
#: 355e1c85d29249c3af30cdc9b5ed5770
msgid ""
"and enjoy the training process. To make changes to your training, you can"
" modify the arguments in the training command to adjust the "
"hyperparameters. One argument to note is ``cutoff_len``, which is the "
"maximum length of the training data. Control this parameter to avoid OOM "
"error."
msgstr ""
"并享受训练过程。若要调整您的训练，您可以通过修改训练命令中的参数来调整超参数。其中一个需要注意的参数是 ``cutoff_len`` "
"，它代表训练数据的最大长度。通过控制这个参数，可以避免出现OOM（内存溢出）错误。"

#: ../../source/training/SFT/llama_factory.rst:173
#: b6910242cca94fe1b5cda3da730dc984
msgid "Merge LoRA"
msgstr "合并LoRA"

#: ../../source/training/SFT/llama_factory.rst:175
#: f1ef4d16d0b1495c99467a7783430bad
msgid ""
"If you train your model with LoRA, you probably need to merge adapter "
"parameters to the main branch. Run the following command to perform the "
"merging of LoRA adapters."
msgstr "如果你使用 LoRA 训练模型，可能需要将adapter参数合并到主分支中。请运行以下命令以执行 LoRA adapter 的合并操作。"

#: ../../source/training/SFT/llama_factory.rst:191
#: fea078e8ebd64be7acd9d4d8afd43fec
msgid "Conclusion"
msgstr "结语"

#: ../../source/training/SFT/llama_factory.rst:193
#: 47b180aa689a493488dfb60b490c1de7
msgid ""
"The above content is the simplest way to use LLaMA-Factory to train Qwen."
" Feel free to dive into the details by checking the official repo!"
msgstr "上述内容是使用LLaMA-Factory训练Qwen的最简单方法。 欢迎通过查看官方仓库深入了解详细信息！"

