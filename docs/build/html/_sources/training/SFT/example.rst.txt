Supervised Finetuning Example
====================================================

Here we provide a very simple script, which is revised from the training
script in `Fastchat <https://github.com/lm-sys/FastChat>`__. The
script is used to finetune Qwen with Hugging Face Trainer. You can check
the script
`here <https://github.com/QwenLM/Qwen1.5/blob/main/finetune.py>`__. This
script for supervised finetuning (SFT) has the following features:

-  Support single-GPU and multi-GPU training;
-  Support full-parameter tuning,
   `LoRA <https://arxiv.org/abs/2106.09685>`__, and
   `Q-LoRA <https://arxiv.org/abs/2305.14314>`__.

In the following, we introduce more details about the usage of the
script.

Installation
------------

Before you start, make sure you have installed the following packages:

.. code:: bash

   pip install peft deepspeed optimum accelerate

Data Preparation
----------------

For data preparation, we advise you to organize the data in a jsonl
file, where each line is a dictionary as demonstrated below:

.. code:: json

   {
       "type": "chatml",
       "messages": [
           {
               "role": "system",
               "content": "You are a helpful assistant."
           },
           {
               "role": "user",
               "content": "Tell me something about large language models."
           },
           {
               "role": "assistant",
               "content": "Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks..."
           }
       ],
       "source": "unknown"
   }

.. code:: json

   {
       "type": "chatml",
       "messages": [
           {
               "role": "system",
               "content": "You are a helpful assistant."
           },
           {
               "role": "user",
               "content": "What is your name?"
           },
           {
               "role": "assistant",
               "content": "My name is Qwen."
           }
       ],
       "source": "self-made"
   }

Above are two examples of each data sample in the dataset. Each sample
is a JSON object with the following fields: ``type``, ``messages`` and
``source``. ``messages`` is required while the others are optional for
you to label your data format and data source. The ``messages`` field is
a list of JSON objects, each of which has two fields: ``role`` and
``content``. ``role`` can be ``system``, ``user``, or ``assistant``.
``content`` is the text of the message. ``source`` is the source of the
data, which can be ``self-made``, ``alpaca``, ``open-hermes``, or any
other string.

To make the jsonl file, you can use ``json`` to save a list of
dictionaries to the jsonl file:

.. code:: python

   import json

   with open('data.jsonl', 'w') as f:
       for sample in samples:
           f.write(json.dumps(sample) + '\n')

Quickstart
----------

For you to start finetuning quickly, we directly provide shell scripts
for you to run without paying attention to details. While you need
different hyperparameters for different types of training, e.g.,
single-GPU / multi-GPU training, full-parameter tuning, LoRA, or Q-LoRA.

The scripts are demonstrated below. Choose one based on your
requirements:

.. code:: bash

   bash finetune/<script_path> -m <model_path> -d <data_path> --deepspeed <config_path>

Specify the ``<script_path>`` for your training:

-  ``full.sh``: full-parameter tuning;
-  ``lora.sh``: LoRA;
-  ``qlora.sh``: Q-LoRA.

Specify the ``<model_path>`` for your model, ``<data_path>`` for your
data, and ``<config_path>`` for your deepspeed configuration. This is
the simplest way to start finetuning. If you want to change more
hyperparameters, you can dive into the script and modify those
parameters.

Advanced Usages
---------------

In this section, we introduce the details of the scripts, including the
core python script as well as the corresponding shell scripts.

Shell Scripts
~~~~~~~~~~~~~

Before we introduce the python code, we provide a brief introduction to
the shell script with commands. We provide some guidance inside the
shell script and here we take ``lora.sh`` as an example.

To set up the environment variables for distributed training (or
single-GPU training), specify the following variables:
``GPUS_PER_NODE``, ``NNODES``, ``NODE_RANK``, ``MASTER_ADDR``, and
``MASTER_PORT``. No need to worry too much about them as we provide the
default settings for you. In the command, you can pass in the argument
``-m`` and ``-d`` to specify the model path and data path, respectively.
You can also pass in the argument ``--deepspeed`` to specify the
deepspeed configuration file. We provide two configuration files for
ZeRO2 and ZeRO3, and you can choose one based on your requirements. In
most cases, we recommend using ZeRO3 for multi-GPU training except for
Q-LoRA, where we recommend using ZeRO2.

There are a series of hyperparameters to tune. Passing in ``--bf16`` or
``--fp16`` to specify the precision for mixed precision training. Note
that if you use GPTQ models for Q-LoRA, you need to specify ``--fp16``.
The other significant hyperparameters include:

-  ``--output_dir``: the path of your output models or adapters.
-  ``--num_train_epochs``: the number of training epochs.
-  ``--gradient_accumulation_steps``: the number of gradient
   accumulation steps.
-  ``--per_device_train_batch_size``: the batch size per GPU for
   training, and the total batch size is equalt to
   ``per_device_train_batch_size`` :math:`\times` ``number_of_gpus``
   :math:`\times` ``gradient_accumulation_steps``.
-  ``--learning_rate``: the learning rate.
-  ``--warmup_steps``: the number of warmup steps.
-  ``--lr_scheduler_type``: the type of learning rate scheduler.
-  ``--weight_decay``: the value of weight decay.
-  ``--adam_beta2``: the value of :math:`\beta_2` in Adam.
-  ``--model_max_length``: the maximum sequence length.
-  ``--use_lora``: whether to use LoRA. Adding ``--q_lora`` can enable
   Q-LoRA.
-  ``--gradient_checkpointing``: whether to use gradient checkpointing.

Python Script
~~~~~~~~~~~~~

In this script, we mainly use ``trainer`` from HF and ``peft`` to train
our models. We also use ``deepspeed`` to accelerate the training
process. The script is very simple and easy to understand.

.. code:: python

   @dataclass
   class TrainingArguments(transformers.TrainingArguments):
       cache_dir: Optional[str] = field(default=None)
       optim: str = field(default="adamw_torch")
       model_max_length: int = field(
           default=8192,
           metadata={
               "help": "Maximum sequence length. Sequences will be right padded (and possibly truncated)."
           },
       )
       use_lora: bool = False


   @dataclass
   class LoraArguments:
       lora_r: int = 64
       lora_alpha: int = 16
       lora_dropout: float = 0.05
       lora_target_modules: List[str] = field(
           default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "gate_proj", "down_proj"]
       )
       lora_weight_path: str = ""
       lora_bias: str = "none"
       q_lora: bool = False

The classes for arguments allow you to specify hyperparameters for
model, data, training, and additionally LoRA if you use LoRA or Q-LoRA
to train your model. Specifically, ``model-max-length`` is a key
hyperparameter that determines your maximum sequence length of your
training data.

``LoRAArguments`` includes the hyperparameters for LoRA or Q-LoRA:

-  ``lora_r``: the rank for LoRA;
-  ``lora_alpha``: the alpha value for LoRA;
-  ``lora_dropout``: the dropout rate for LoRA;
-  ``lora_target_modules``: the target modules for LoRA. By default we
   tune all linear layers;
-  ``lora_weight_path``: the path to the weight file for LoRA;
-  ``lora_bias``: the bias for LoRA;
-  ``q_lora``: whether to use Q-LoRA. This is the key hyperparameter for
   Q-LoRA. Pay attention to the base language model that you use. We
   advise you to use our provided GPTQ or AWQ models, but you can also
   use the base model with ``bitsandbytes``.

The method ``safe_save_model_for_hf_trainer``, which uses
``get_peft_state_maybe_zero_3``, helps tackle the problems in saving
models trained either with or without ZeRO3.

For data preprocessing, we use ``preprocess`` to organize the data.
Specifically, we apply our ChatML template to the texts. If you prefer
other chat templates, you can use others, e.g., by still applying
``apply_chat_template()`` with another tokenizer. The chat template is
stored in the ``tokenizer_config.json`` in the HF repo. Additionally, we
pad the sequence of each sample to the maximum length for training.

Then we utilize ``make_supervised_data_module`` by using
``SupervisedDataset`` or ``LazySupervisedDataset`` to build the dataset.

The ``train`` method is the key to the training. In general, it loads
the tokenizer and model with ``AutoTokenizer.from_pretrained()`` and
``AutoModelForCausalLM.from_pretrained()``. If we use LoRA, the method
will initialize LoRA configuration with ``LoraConfig``. If we apply
Q-LoRA, we should use ``prepare_model_for_kbit_training``. Then we leave
the following efforts to ``trainer`` and have a cup of coffee!

Next Step
---------

Now, you are able to use a very simple script to perform different types
of SFT. Alternatively, you can use more advanced training libraries,
such as
`Axolotl <https://github.com/OpenAccess-AI-Collective/axolotl>`__ or
`LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__, to enjoy
more functionalities. To take a step forward, after SFT, you can
consider RLHF to align your model to human preferences! Stay tuned for
our next tutorial on RLHF!
