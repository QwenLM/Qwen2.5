# æ•ˆç‡è¯„ä¼°

æœ¬æ–‡ä»‹ç»Qwen2.5ç³»åˆ—æ¨¡å‹ï¼ˆåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹ï¼‰çš„æ•ˆç‡æµ‹è¯•æµç¨‹ï¼Œè¯¦ç»†æŠ¥å‘Šå¯å‚è€ƒ [Qwen2.5æ¨¡å‹æ•ˆç‡è¯„ä¼°æŠ¥å‘Š](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html)ã€‚

## 1. æ¨¡å‹èµ„æº

å¯¹äºæ‰˜ç®¡åœ¨HuggingFaceä¸Šçš„æ¨¡å‹ï¼Œå¯å‚è€ƒ [Qwen2.5æ¨¡å‹-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)ã€‚

å¯¹äºæ‰˜ç®¡åœ¨ModelScopeä¸Šçš„æ¨¡å‹ï¼Œå¯å‚è€ƒ [Qwen2.5æ¨¡å‹-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)ã€‚


## 2. ç¯å¢ƒå®‰è£…

ä½¿ç”¨HuggingFace transformersæ¨ç†ï¼Œå®‰è£…ç¯å¢ƒå¦‚ä¸‹ï¼š

```shell
conda create -n qwen_perf_transformers python=3.10
conda activate qwen_perf_transformers

pip install torch==2.3.1
pip install git+https://github.com/AutoGPTQ/AutoGPTQ.git@v0.7.1
pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.5.8
pip install -r requirements-perf-transformers.txt
```

> [!Important]
> - å¯¹äº `flash-attention`ï¼Œæ‚¨å¯ä»¥ä» [GitHub å‘å¸ƒé¡µé¢](https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.8) ä½¿ç”¨é¢„ç¼–è¯‘çš„ wheel åŒ…è¿›è¡Œå®‰è£…ï¼Œæˆ–è€…ä»æºä»£ç å®‰è£…ï¼Œåè€…éœ€è¦ä¸€ä¸ªå…¼å®¹çš„ CUDA ç¼–è¯‘å™¨ã€‚
>   - å®é™…ä¸Šï¼Œæ‚¨å¹¶ä¸éœ€è¦å•ç‹¬å®‰è£… `flash-attention`ã€‚å®ƒå·²ç»è¢«é›†æˆåˆ°äº† `torch` ä¸­ä½œä¸º `sdpa` çš„åç«¯å®ç°ã€‚
> - è‹¥è¦ä½¿ `auto_gptq` ä½¿ç”¨é«˜æ•ˆçš„å†…æ ¸ï¼Œæ‚¨éœ€è¦ä»æºä»£ç å®‰è£…ï¼Œå› ä¸ºé¢„ç¼–è¯‘çš„ wheel åŒ…ä¾èµ–äºä¸ä¹‹ä¸å…¼å®¹çš„ `torch` ç‰ˆæœ¬ã€‚ä»æºä»£ç å®‰è£…åŒæ ·éœ€è¦ä¸€ä¸ªå…¼å®¹çš„ CUDA ç¼–è¯‘å™¨ã€‚
> - è‹¥è¦ä½¿ `autoawq` ä½¿ç”¨é«˜æ•ˆçš„å†…æ ¸ï¼Œæ‚¨éœ€è¦å®‰è£… `autoawq-kernels`ï¼Œè¯¥ç»„ä»¶åº”å½“ä¼šè‡ªåŠ¨å®‰è£…ã€‚å¦‚æœæœªè‡ªåŠ¨å®‰è£…ï¼Œè¯·è¿è¡Œ `pip install autoawq-kernels` è¿›è¡Œæ‰‹åŠ¨å®‰è£…ã€‚


ä½¿ç”¨vLLMæ¨ç†ï¼Œå®‰è£…ç¯å¢ƒå¦‚ä¸‹ï¼š

```shell
conda create -n qwen_perf_vllm python=3.10
conda activate qwen_perf_vllm

pip install -r requirements-perf-vllm.txt
```


## 3. æ‰§è¡Œæµ‹è¯•

ä¸‹é¢ä»‹ç»ä¸¤ç§æ‰§è¡Œæµ‹è¯•çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ˜¯ä½¿ç”¨è„šæœ¬æµ‹è¯•å’Œä½¿ç”¨Speed Benchmarkå·¥å…·è¿›è¡Œæµ‹è¯•ã€‚

### æ–¹æ³•1ï¼šä½¿ç”¨Speed Benchmarkå·¥å…·æµ‹è¯•

ä½¿ç”¨[EvalScope](https://github.com/modelscope/evalscope)å¼€å‘çš„Speed Benchmarkå·¥å…·è¿›è¡Œæµ‹è¯•ï¼Œæ”¯æŒè‡ªåŠ¨ä»modelscopeä¸‹è½½æ¨¡å‹å¹¶è¾“å‡ºæµ‹è¯•ç»“æœï¼Œä¹Ÿæ”¯æŒæŒ‡å®šæ¨¡å‹æœåŠ¡çš„urlè¿›è¡Œæµ‹è¯•ï¼Œå…·ä½“è¯·å‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/speed_benchmark.html)ã€‚

**å®‰è£…ä¾èµ–**
```shell
pip install 'evalscope[perf]' -U
```

#### HuggingFace transformersæ¨ç†

æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š
```shell
CUDA_VISIBLE_DEVICES=0 evalscope perf \
 --parallel 1 \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --attn-implementation flash_attention_2 \
 --log-every-n-query 5 \
 --connect-timeout 6000 \
 --read-timeout 6000 \
 --max-tokens 2048 \
 --min-tokens 2048 \
 --api local \
 --dataset speed_benchmark 
```

#### vLLMæ¨ç†

```shell
CUDA_VISIBLE_DEVICES=0 evalscope perf \
 --parallel 1 \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --log-every-n-query 1 \
 --connect-timeout 60000 \
 --read-timeout 60000\
 --max-tokens 2048 \
 --min-tokens 2048 \
 --api local_vllm \
 --dataset speed_benchmark
```

#### å‚æ•°è¯´æ˜
- `--parallel` è®¾ç½®å¹¶å‘è¯·æ±‚çš„workeræ•°é‡ï¼Œéœ€å›ºå®šä¸º1ã€‚
- `--model` æµ‹è¯•æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿå¯ä¸ºæ¨¡å‹IDï¼Œæ”¯æŒè‡ªåŠ¨ä»modelscopeä¸‹è½½æ¨¡å‹ï¼Œä¾‹å¦‚Qwen/Qwen2.5-0.5B-Instructã€‚
- `--attn-implementation` è®¾ç½®attentionå®ç°æ–¹å¼ï¼Œå¯é€‰å€¼ä¸ºflash_attention_2|eager|sdpaã€‚
- `--log-every-n-query`: è®¾ç½®æ¯nä¸ªè¯·æ±‚æ‰“å°ä¸€æ¬¡æ—¥å¿—ã€‚
- `--connect-timeout`: è®¾ç½®è¿æ¥è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’ã€‚
- `--read-timeout`: è®¾ç½®è¯»å–è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’ã€‚
- `--max-tokens`: è®¾ç½®æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œå•ä½ä¸ºtokenã€‚
- `--min-tokens`: è®¾ç½®æœ€å°è¾“å‡ºé•¿åº¦ï¼Œå•ä½ä¸ºtokenï¼›ä¸¤ä¸ªå‚æ•°åŒæ—¶è®¾ç½®ä¸º2048åˆ™æ¨¡å‹å›ºå®šè¾“å‡ºé•¿åº¦ä¸º2048ã€‚
- `--api`: è®¾ç½®æ¨ç†æ¥å£ï¼Œæœ¬åœ°æ¨ç†å¯é€‰å€¼ä¸ºlocal|local_vllmã€‚
- `--dataset`: è®¾ç½®æµ‹è¯•æ•°æ®é›†ï¼Œå¯é€‰å€¼ä¸ºspeed_benchmark|speed_benchmark_longã€‚

#### æµ‹è¯•ç»“æœ

æµ‹è¯•ç»“æœè¯¦è§`outputs/{model_name}/{timestamp}/speed_benchmark.json`æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰è¯·æ±‚ç»“æœå’Œæµ‹è¯•å‚æ•°ã€‚

### æ–¹æ³•2ï¼šä½¿ç”¨è„šæœ¬æµ‹è¯•

#### HuggingFace transformersæ¨ç†

- ä½¿ç”¨HuggingFace hub

```shell
python speed_benchmark_transformers.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --outputs_dir outputs/transformers

# æŒ‡å®šHF_ENDPOINT
HF_ENDPOINT=https://hf-mirror.com python speed_benchmark_transformers.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --outputs_dir outputs/transformers
```

- ä½¿ç”¨ModelScope hub

```shell
python speed_benchmark_transformers.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformers
```

å‚æ•°è¯´æ˜ï¼š

    `--model_id_or_path`: æ¨¡å‹IDæˆ–æœ¬åœ°è·¯å¾„ï¼Œ å¯é€‰å€¼å‚è€ƒ`æ¨¡å‹èµ„æº`ç« èŠ‚  
    `--context_length`: è¾“å…¥é•¿åº¦ï¼Œå•ä½ä¸ºtokenæ•°ï¼›å¯é€‰å€¼ä¸º1, 6144, 14336, 30720, 63488, 129024ï¼›å…·ä½“å¯å‚è€ƒ`Qwen2.5æ¨¡å‹æ•ˆç‡è¯„ä¼°æŠ¥å‘Š`  
    `--generate_length`: ç”Ÿæˆtokenæ•°é‡ï¼›é»˜è®¤ä¸º2048
    `--gpus`: ç­‰ä»·äºç¯å¢ƒå˜é‡CUDA_VISIBLE_DEVICESï¼Œä¾‹å¦‚`0,1,2,3`ï¼Œ`4,5`  
    `--use_modelscope`: å¦‚æœè®¾ç½®è¯¥å€¼ï¼Œåˆ™ä½¿ç”¨ModelScopeåŠ è½½æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨HuggingFace  
    `--outputs_dir`: è¾“å‡ºç›®å½•ï¼Œ é»˜è®¤ä¸º`outputs/transformers`  


#### vLLMæ¨ç†

- ä½¿ç”¨HuggingFace hub

```shell
python speed_benchmark_vllm.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --max_model_len 32768 --gpus 0 --gpu_memory_utilization 0.9 --outputs_dir outputs/vllm

# æŒ‡å®šHF_ENDPOINT
HF_ENDPOINT=https://hf-mirror.com python speed_benchmark_vllm.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --max_model_len 32768 --gpus 0 --gpu_memory_utilization 0.9 --outputs_dir outputs/vllm
```

- ä½¿ç”¨ModelScope hub

```shell
python speed_benchmark_vllm.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --max_model_len 32768 --gpus 0 --use_modelscope --gpu_memory_utilization 0.9 --outputs_dir outputs/vllm
```

å‚æ•°è¯´æ˜ï¼š

    `--model_id_or_path`: æ¨¡å‹IDæˆ–æœ¬åœ°è·¯å¾„ï¼Œ å¯é€‰å€¼å‚è€ƒ`æ¨¡å‹èµ„æº`ç« èŠ‚  
    `--context_length`: è¾“å…¥é•¿åº¦ï¼Œå•ä½ä¸ºtokenæ•°ï¼›å¯é€‰å€¼ä¸º1, 6144, 14336, 30720, 63488, 129024ï¼›å…·ä½“å¯å‚è€ƒ`Qwen2.5æ¨¡å‹æ•ˆç‡è¯„ä¼°æŠ¥å‘Š`  
    `--generate_length`: ç”Ÿæˆtokenæ•°é‡ï¼›é»˜è®¤ä¸º2048
    `--max_model_len`: æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œå•ä½ä¸ºtokenæ•°ï¼›é»˜è®¤ä¸º32768  
    `--gpus`: ç­‰ä»·äºç¯å¢ƒå˜é‡CUDA_VISIBLE_DEVICESï¼Œä¾‹å¦‚`0,1,2,3`ï¼Œ`4,5`   
    `--use_modelscope`: å¦‚æœè®¾ç½®è¯¥å€¼ï¼Œåˆ™ä½¿ç”¨ModelScopeåŠ è½½æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨HuggingFace  
    `--gpu_memory_utilization`: GPUå†…å­˜åˆ©ç”¨ç‡ï¼Œå–å€¼èŒƒå›´ä¸º(0, 1]ï¼›é»˜è®¤ä¸º0.9  
    `--outputs_dir`: è¾“å‡ºç›®å½•ï¼Œ é»˜è®¤ä¸º`outputs/vllm`  
    `--enforce_eager`: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼ï¼›é»˜è®¤ä¸ºFalse  

#### æµ‹è¯•ç»“æœ

æµ‹è¯•ç»“æœè¯¦è§`outputs`ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œé»˜è®¤åŒ…æ‹¬`transformers`å’Œ`vllm`ä¸¤ä¸ªç›®å½•ï¼Œåˆ†åˆ«å­˜æ”¾HuggingFace transformerså’ŒvLLMçš„æµ‹è¯•ç»“æœã€‚

## æ³¨æ„äº‹é¡¹

1. å¤šæ¬¡æµ‹è¯•ï¼Œå–å¹³å‡å€¼ï¼Œå…¸å‹å€¼ä¸º3æ¬¡
2. æµ‹è¯•å‰è¯·ç¡®ä¿GPUå¤„äºç©ºé—²çŠ¶æ€ï¼Œé¿å…å…¶ä»–ä»»åŠ¡å½±å“æµ‹è¯•ç»“æœ


